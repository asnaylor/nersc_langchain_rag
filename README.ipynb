{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4306e65e-0efb-42c0-a90b-e39df64e8d44",
   "metadata": {},
   "source": [
    "# NERSC LangChain RAG \n",
    "\n",
    "## Overview\n",
    "Retrieval-Augmented Generation (RAG) is a technique that combines the strengths of retrieval-based and generation-based models to enhance the quality of generated content. This Jupyter notebook demo will walk you through the steps to set up and run a RAG model using NERSC's supercompters. For more explanation about RAG, visit this [blog](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/).\n",
    "\n",
    "## Prerequisites\n",
    "1. **NERSC Account**: Ensure you have an active NERSC account with sufficient \"Node Hours\" to execute GPU compute.\n",
    "2. **Hugging Face Access Token**: For models that require special access, create a Hugging Face account if you don't already have one. Follow [these instructions](https://huggingface.co/docs/hub/en/security-tokens) to generate a user access token and update the `HF_TOKEN` variable with your token.\n",
    "3. **Nvidia NGC API Key**: To access specific models and containers, create an Nvidia Developer account if you don't have one. Follow [these instructions](https://docs.nvidia.com/ai-enterprise/deployment/spark-rapids-accelerator/latest/appendix-ngc.html) to generate an NGC API key and update the `NGC_API_KEY` variable with your key.\n",
    "\n",
    "## Installation\n",
    "To get started on a NERSC supercomputer, follow these steps to clone the repository, install the required dependencies, and pull the necessary containers:\n",
    "\n",
    "1. Clone the Repository:\n",
    "```bash\n",
    "git clone https://github.com/yourusername/nersc_langchain_rag.git\n",
    "cd nersc_langchain_rag\n",
    "```\n",
    "\n",
    "2. Create a Conda Environment and Jupyter Kernel:\n",
    "```bash\n",
    "ENV_DIR=$SCRATCH/langchain\n",
    "module load conda\n",
    "mamba create --prefix ${ENV_DIR} python=3.12 ipykernel -y\n",
    "mamba activate ${ENV_DIR}\n",
    "python -m ipykernel install \\\n",
    "    --user --name langchain --display-name LangChain\n",
    "pip install langchain langchain-openai langchain-community langchain-nvidia-ai-endpoints pypdf\n",
    "mamba install faiss-gpu -y\n",
    "```\n",
    "\n",
    "3. Pull the containers:\n",
    "```bash\n",
    "#The vLLM image is already on shifter - vllm/vllm-openai:v0.7.3\n",
    "\n",
    "#Provide podman with NGC_API_KEY\n",
    "echo $NGC_API_KEY | podman-hpc login nvcr.io --username '$oauthtoken' --password-stdin\n",
    "\n",
    "#Download embedding and ranking containers from NGC\n",
    "podman-hpc pull nvcr.io/nim/nvidia/nv-embedqa-e5-v5:1.5.0\n",
    "podman-hpc pull nvcr.io/nim/nvidia/nv-rerankqa-mistral-4b-v3:1.0.2\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "Once you have completed the setup, you can start running the Jupyter Notebook on [NERSC JupyterHub](https://jupyter.nersc.gov/) to execute the RAG model (select the \"LangChain\" Python kernel). The notebook contains instructions and code snippets to help you understand the process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c9199b-2e05-4a50-8c37-ab47ec30158a",
   "metadata": {},
   "source": [
    "# Notebook\n",
    "\n",
    "\n",
    "## Overview\n",
    "This RAG example will be creating a sophisticated question-answering (Q&A) chatbot focused on the [NERSC 10-Year strategic plan](https://www.nersc.gov/about/nersc-10-year-strategic-plan/). This document outlines NERSCâ€™s mission, goals, science drivers, planned initiatives, and technology strategy for the coming decade. By combining advanced language models and retrieval techniques, the chatbot will be able to provide accurate and contextually relevant answers.\n",
    "\n",
    "The chatbot chain is constructed using the LangChain library and integrates the following models:\n",
    "- **Meta Llama 3.3 70B Instruct Model**: Served via vLLM, this model provides the foundational language generation capabilities.\n",
    "- **NVIDIA Retrieval QA E5 Embedding Model**: Utilized for embedding-based retrieval, this model is served by NVIDIA NIM.\n",
    "- **NVIDIA Retrieval QA Mistral 4B Reranking Model**: Employed for reranking retrieved results, this model is also served by NVIDIA NIM.\n",
    "\n",
    "These models work together to enhance the chatbot's ability to understand and generate responses based on the content of the NERSC 10-Year strategic plan. The embedding model helps in retrieving relevant chunks of text, while the reranking model ensures that the most relevant information is prioritized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16cfa47-00d3-413a-aa6d-c05d1c03d768",
   "metadata": {},
   "source": [
    "## LLM\n",
    "\n",
    "In this section, we will deploy the foundational language model using the `deploy_llm` command. This command will run the model in the background on NERSC's GPU compute resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a686d134-f99c-4a50-9b63-ce7d2d45aaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started process with PID: 232266\n"
     ]
    }
   ],
   "source": [
    "from utils import allocate_gpu_resources, generate_api_key\n",
    "\n",
    "# Generate an API key for the LLM service\n",
    "llm_api_key = generate_api_key()\n",
    "\n",
    "# Command to deploy the LLM using vLLM\n",
    "llm_command = (\n",
    "    \"srun -n 1 --cpus-per-task=128 --gpus-per-task=4 \"\n",
    "    \"  shifter \"\n",
    "    \"    --image=vllm/vllm-openai:v0.7.3 \"\n",
    "    \"    --module=gpu,nccl-plugin \"\n",
    "    \"    --env=HF_TOKEN=\\\"$(cat $HOME/.hf_token)\\\" \"\n",
    "    \"    --env=HF_HOME=\\\"$SCRATCH/huggingface/\\\" \"\n",
    "    \"        vllm serve meta-llama/Llama-3.3-70B-Instruct \"\n",
    "    f\"             --api-key {llm_api_key} --tensor-parallel-size 4\"\n",
    ")\n",
    "\n",
    "# Allocate GPU resources via Slurm and start the LLM process\n",
    "llm_process = allocate_gpu_resources(\n",
    "    account=\"dasrepo\",\n",
    "    num_gpus=4,\n",
    "    queue=\"interactive\",\n",
    "    time=\"01:00:00\",\n",
    "    job_name=\"vLLM_RAG\",\n",
    "    commands=llm_command\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec9f866-16df-413c-98fc-a39cfdcc9f09",
   "metadata": {},
   "source": [
    "Check the job has started and once started get vLLM address and check the model has been loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "513da2cc-9434-4da8-9c78-e975a080dac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          37406791 urgent_gp vLLM_RAG asnaylor  R       0:03      1 nid200392\n"
     ]
    }
   ],
   "source": [
    "!squeue --me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cb71a31-d02f-4dfd-8f0e-ebd3c71af068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while checking the service status: HTTPConnectionPool(host='nid200392.chn.perlmutter.nersc.gov', port=8000): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f3963bfc770>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Service is not up yet. Checking again in 60 seconds...\n",
      "An error occurred while checking the service status: HTTPConnectionPool(host='nid200392.chn.perlmutter.nersc.gov', port=8000): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f396289e8a0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Service is not up yet. Checking again in 60 seconds...\n",
      "An error occurred while checking the service status: HTTPConnectionPool(host='nid200392.chn.perlmutter.nersc.gov', port=8000): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f396289f0e0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Service is not up yet. Checking again in 60 seconds...\n",
      "Service is up.\n"
     ]
    }
   ],
   "source": [
    "from utils import get_node_address, monitor_service_status\n",
    "\n",
    "# Get the vLLM address\n",
    "vLLM_address=f\"http://{get_node_address('vLLM_RAG')}.chn.perlmutter.nersc.gov:8000/v1\"\n",
    "\n",
    "# Check if the service is up and the model is loaded\n",
    "monitor_service_status(vLLM_address, endpoint=\"/models\", api_key=llm_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2f75cb-42ac-4cf2-9791-25bf5851592a",
   "metadata": {},
   "source": [
    "## Embedding + Reranking\n",
    "In this section, we will deploy the embedding and reranking models. These models will be used to retrieve and rank relevant information from the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f887da62-7d00-46b8-9030-4ac8c3e7961f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started process with PID: 233004\n"
     ]
    }
   ],
   "source": [
    "# Pre-command to set up the environment and cache directory\n",
    "pre_command = \"mkdir -p ${SCRATCH}/nim_cache; export PODMANHPC_PODMAN_BIN=/global/common/shared/das/podman-4.7.0/bin/podman; \"\n",
    "\n",
    "# Command to deploy the embedding model\n",
    "embed_command = (\n",
    "    \"(srun -n 1 --cpus-per-task=32 --gpus-per-task=1 --overlap\"\n",
    "    \"  podman-hpc run --rm -it --gpu --userns keep-id:uid=1000,gid=1000 \"\n",
    "    \"    --volume \\\"${SCRATCH}/nim_cache:/opt/nim/.cache\\\" \"\n",
    "    \"    --env=NGC_API_KEY=\\\"$(grep apikey ~/.ngc/config | awk '{printf $3}')\\\" \"\n",
    "    \"    --env=NIM_HTTP_API_PORT=\\\"8000\\\" \"\n",
    "    \"    -p \\\"8010:8000\\\" \"\n",
    "    \"        nvcr.io/nim/nvidia/nv-embedqa-e5-v5:1.5.0 ) & \"\n",
    ")\n",
    "\n",
    "# Command to deploy the reranking model\n",
    "rerank_command = (\n",
    "    \"(srun -n 1 --cpus-per-task=32 --gpus-per-task=1 --overlap\"\n",
    "    \"  podman-hpc run --rm -it --gpu --userns keep-id:uid=1000,gid=1000 \"\n",
    "    \"    --volume \\\"${SCRATCH}/nim_cache:/opt/nim/.cache\\\"\"\n",
    "    \"    --env=NGC_API_KEY=\\\"$(grep apikey ~/.ngc/config | awk '{printf $3}')\\\" \"\n",
    "    \"    --env=NIM_HTTP_API_PORT=\\\"8000\\\"\"\n",
    "    \"    -p \\\"8020:8000\\\" \"\n",
    "    \"        nvcr.io/nim/nvidia/nv-rerankqa-mistral-4b-v3:1.0.2 ) & \"\n",
    ")\n",
    "\n",
    "# Combine the commands and wait for both processes to complete\n",
    "embed_rerank_command = pre_command + embed_command + rerank_command + \"wait\"\n",
    "\n",
    "# Allocate GPU resources via Slurm and start the embedding and reranking processes\n",
    "embed_rerank_process = allocate_gpu_resources(\n",
    "    account=\"dasrepo\",\n",
    "    num_gpus=2,\n",
    "    queue=\"shared_interactive\",\n",
    "    time=\"01:00:00\",\n",
    "    job_name=\"embed_rerank_RAG\",\n",
    "    commands=embed_rerank_command\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b8a613-2759-44aa-ba98-98af40491a7d",
   "metadata": {},
   "source": [
    "Check the job has started and once started get addresses and check the models have been loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae4bbaff-3d72-48b6-b418-622a05819512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          37406794 shared_ur embed_re asnaylor  R       0:12      1 nid200325\n",
      "          37406791 urgent_gp vLLM_RAG asnaylor  R       0:24      1 nid200392\n"
     ]
    }
   ],
   "source": [
    "!squeue --me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65e2a554-ed2c-468e-9d63-c3a924e344d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the addresses for embedding and reranking services\n",
    "embed_rerank_address=f\"http://{get_node_address('embed_rerank_RAG')}.chn.perlmutter.nersc.gov\"\n",
    "embed_address=f\"{embed_rerank_address}:8010/v1\"\n",
    "rerank_address=f\"{embed_rerank_address}:8020/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "741eb6ae-a1f6-4f05-b545-7062eaa736fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service is up.\n"
     ]
    }
   ],
   "source": [
    "# Check if the embedding service is up\n",
    "monitor_service_status(embed_address, endpoint=\"/health/ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23fef905-1d69-4ff4-ab59-3add8f2a0c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service is up.\n"
     ]
    }
   ],
   "source": [
    "# Check if the reranking service is up\n",
    "monitor_service_status(rerank_address, endpoint=\"/health/ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a103f384-5033-4bb6-845a-ebf57da44805",
   "metadata": {},
   "source": [
    "## Connect to services\n",
    "In this section, we will connect to the deployed services using the LangChain library. This will involve initializing the language model, embedding model, and reranking model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58c0fe83-8f1a-4618-92eb-fafb8ef99e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings, NVIDIARerank\n",
    "\n",
    "# Initialize the language model with the vLLM address and API key\n",
    "llm = ChatOpenAI(\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    base_url=vLLM_address,\n",
    "    api_key=llm_api_key\n",
    ")\n",
    "\n",
    "# Initialize the embedding model with the embedding service address\n",
    "embedder = NVIDIAEmbeddings(\n",
    "    model=\"nvidia/nv-embedqa-e5-v5\",\n",
    "    base_url=embed_address\n",
    ")\n",
    "\n",
    "# Initialize the reranking model with the reranking service address\n",
    "reranker = NVIDIARerank(\n",
    "    model=\"nvidia/nv-rerankqa-mistral-4b-v3\",\n",
    "    base_url=rerank_address\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce519d81-11d1-4641-9034-f7da362e1980",
   "metadata": {},
   "source": [
    "## Process Document\n",
    "In this section, we will download and process the NERSC 10-Year strategic plan PDF document. This involves downloading the document, splitting it into manageable chunks, and embedding the chunks for retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65917109-e0ea-45cf-9043-76ee4c884b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists at nersc_10yr_plan.pdf. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "from utils import download_pdf\n",
    "\n",
    "# URL and local path for the NERSC 10-Year strategic plan PDF\n",
    "nersc_10yr_plan_url = 'https://www.nersc.gov/assets/Annual-Reports/2024-2034-NERSC-10-yr-Strategic-Plan-v2.pdf'\n",
    "nersc_10yr_plan_pdf = 'nersc_10yr_plan.pdf'\n",
    "\n",
    "# Download the PDF document\n",
    "download_pdf(nersc_10yr_plan_url, nersc_10yr_plan_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9461d52-9dc8-4571-a9cd-8769cff5828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "# Load the PDF document\n",
    "loader = PyPDFLoader(nersc_10yr_plan_pdf)\n",
    "document = loader.load()\n",
    "\n",
    "# Split the document into smaller chunks\n",
    "document_chunks = text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0557ef21-209e-4b64-802e-076b2b64382c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages:  28\n",
      "Number of chunks from the document: 149\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of pages: \", len(document))\n",
    "print(\"Number of chunks from the document:\", len(document_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e07b6a-7aef-4bf2-bf2f-c8fe80e1b3e9",
   "metadata": {},
   "source": [
    "Embed the document chunks using the embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bb57dbd-366b-4a6d-b078-7731aeb13c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Embed the document chunks\n",
    "vectorstore = FAISS.from_documents(document_chunks, embedder)\n",
    "\n",
    "# Save the vector store to a file\n",
    "vector_store_path = f\"{os.getenv('SCRATCH')}/rag_langchain_db_vectorstore.pkl\"\n",
    "with open(vector_store_path, \"wb\") as f:\n",
    "    pickle.dump(vectorstore, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2308cf-fbb7-4edb-8d66-142384cbec70",
   "metadata": {},
   "source": [
    "Set up the retriever with contextual compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6aecb96d-9388-46e0-9891-cfc6c9182a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Initialize the compression retriever\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=reranker, base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78855eeb-08d4-497b-b73d-f060e46beb3c",
   "metadata": {},
   "source": [
    "## Setup chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730e583-d688-43de-a138-469747920717",
   "metadata": {},
   "source": [
    "In this section, we will set up the chain to process the retrieved documents and generate answers. This involves defining the prompt template and creating the QA chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8dad5a3-b943-4d88-a2de-8e78b8d49e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define the prompt template for the chatbot\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are an advanced AI assistant with expertise in extracting and summarizing information from documents. \"\n",
    "            \"Your task is to answer questions based on the content of the provided PDF document. \"\n",
    "            \"Please ensure your responses are accurate, concise, and directly related to the content of the PDF.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Here is the content of the PDF document you will use to answer questions:\\n\\n{pdf_content}\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Based on the content above, please provide a detailed answer to the following question. \"\n",
    "            \"Make sure your answer is comprehensive and references specific information from the PDF content when necessary.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Question: {question}\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create the chat prompt template\n",
    "rag_chat_prompt = ChatPromptTemplate.from_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece66bd9-ad9c-4db1-a2d2-cc8c6d4b9e2b",
   "metadata": {},
   "source": [
    "Define the chain to process the documents and generate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50be7f57-3cd4-4d10-8f07-76ec86b3029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Convert loaded documents into strings by concatenating their content and ignoring metadata\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Define the QA chain\n",
    "qa_chain = (\n",
    "    {\"pdf_content\": compression_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_chat_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40537f7b-f299-4847-a5ed-a1f7ce7da06d",
   "metadata": {},
   "source": [
    "## Q&A with retrieval\n",
    "In this section, we will use the QA chain to answer questions based on the NERSC 10-Year strategic plan. This involves invoking the QA chain with a question and printing the answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d31001b1-e223-48c2-8023-9d8cde6b49e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided PDF content, here is a short bullet point summary of NERSC's AI goals for the next 10 years:\n",
      "\n",
      "* Determine AI capability requirements for NERSC science and HPC system operations within 1-2 years\n",
      "* Produce initial design and prototypes of components for a next-generation NERSC AI services platform within 1-2 years\n",
      "* Seamlessly integrate AI into all science workflows, opening up new potential for scientific discovery, by exploiting innovations in other areas of NERSC's strategy\n",
      "* Leverage AI in automated monitoring, including creating tools and dashboards to prepare for automated monitoring, and deploying automated monitoring on the NERSC-10 system within 3-5 years\n",
      "* Develop system hardware and software that liberates scientists to apply large AI models, including accelerators for pervasive AI, workflow, and data management\n",
      "\n",
      "Note: These goals are part of NERSC's broader strategy to build a pervasive AI ecosystem and achieve a self-driving smart facility within the next 10 years.\n"
     ]
    }
   ],
   "source": [
    "answer = qa_chain.invoke(\n",
    "    \"Write me a short bullet point summary of NERSC's AI goals for the next 10 years\"\n",
    ")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "457e5986-d0c8-4348-875d-6a396cfa2f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the NERSC Strategic Plan FY2024-2032, here is a simple bullet point implementation plan for the AI goals:\n",
      "\n",
      "* **Short-term (1-2 years)**:\n",
      "  * Host foundational AI models and datasets\n",
      "  * Develop intelligent AI-driven interfaces to compute\n",
      "  * Establish a Cross-Team Center Roadmap Committee to review, schedule, and plan AI-related projects\n",
      "  * Develop a plan for center-wide monitoring, including milestones, software roadmap, hardware architecture, cost, and FTE count, in the context of NERSC-10\n",
      "* **Medium-term (3-5 years)**:\n",
      "  * Create tools and dashboards to prepare for automated monitoring, including leveraging AI\n",
      "  * Deploy automated monitoring on the NERSC-10 system\n",
      "  * Develop a Center Roadmap Tool to collect and visualize NERSC projects\n",
      "  * Simplify and automate NERSC-10 interoperability with other data center resources\n",
      "  * Increase ability to support emerging technologies\n",
      "  * Implement continuous improvement to data center without major disruptions to users\n",
      "* **Long-term (6-10 years)**:\n",
      "  * Achieve pervasive AI, where AI is used to accelerate science and HPC wherever possible\n",
      "  * Enable scientists to use AI with human and AI-driven expertise\n",
      "  * Develop applications for science with large-scale, science-informed, robust, transferable models\n",
      "  * Train staff in using modern hardware, software, and techniques to support AI adoption\n",
      "  * Mesh operational efforts seamlessly with forward-looking strategies, including NERSC-11\n",
      "\n",
      "This implementation plan is based on the specific goals and timelines mentioned in the NERSC Strategic Plan FY2024-2032, and is intended to provide a general roadmap for achieving the strategic AI goals outlined in the plan.\n"
     ]
    }
   ],
   "source": [
    "answer = qa_chain.invoke(\n",
    "    \"Based on the NERSC strategic AI goals, create a simple bullet point implentation plan for those goals\"\n",
    ")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be330acd-85b0-4a18-b046-ef085b9b819e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided PDF content, NERSC's AI goals are centered around building a pervasive AI ecosystem that enables scientists to apply large AI models, accelerate scientific discovery, and integrate AI into all science workflows. However, achieving these goals will require overcoming several challenges. Based on the information provided, some of the most important challenges in achieving NERSC's AI goals are:\n",
      "\n",
      "1. **Scaling AI capabilities without significantly increasing staff effort**: The PDF content mentions that \"Expanding the world-class NERSC user experience to meet these challenges (without significantly increasing the NERSC staff effort) will be challenging.\" This indicates that NERSC faces a challenge in scaling its AI capabilities while maintaining its current staff levels.\n",
      "\n",
      "2. **Keeping pace with the fast pace of AI development**: The PDF content states that \"The fast pace of AI development motivates the continuing\" need for innovation and adaptation in NERSC's AI ecosystem. This implies that NERSC must be able to keep pace with the rapid evolution of AI technologies, which can be a significant challenge.\n",
      "\n",
      "3. **Balancing limited resources to prioritize focus areas**: The PDF content mentions that NERSC has identified four focus areas to prioritize the use of limited resources, including \"Impact on science through partnership with users,\" \"System and data center architecture,\" \"Smart green facility,\" and \"Workforce development.\" This suggests that NERSC faces a challenge in allocating its limited resources effectively across these focus areas to achieve its AI goals.\n",
      "\n",
      "4. **Designing and implementing next-generation AI services platforms**: The PDF content mentions that one of NERSC's goals is to \"Produce initial design and prototypes of components for a next-generation NERSC AI services platform\" within the next 1-2 years. This implies that NERSC faces a challenge in designing and implementing a next-generation AI services platform that meets the evolving needs of its users.\n",
      "\n",
      "5. **Integrating AI into all science workflows**: The PDF content states that NERSC's goal is to \"seamlessly integrate AI into all science workflows\" by exploiting innovations in various areas of its strategy. This will require significant efforts to adapt existing workflows, develop new tools and frameworks, and train users to effectively leverage AI capabilities.\n",
      "\n",
      "6. **Providing cutting-edge HPC systems and standardized frameworks for AI**: The PDF content emphasizes the need for \"cutting-edge HPC systems for AI together with standardized frameworks and adaptable tools for use in AI training.\" This implies that NERSC faces a challenge in providing the necessary infrastructure and tools to support AI applications and workflows.\n",
      "\n",
      "Overall, achieving NERSC's AI goals will require addressing these challenges and making significant progress in various areas, including scaling AI capabilities, keeping pace with AI development, prioritizing focus areas, designing next-generation AI services platforms, integrating AI into science workflows, and providing cutting-edge HPC systems and frameworks for AI.\n"
     ]
    }
   ],
   "source": [
    "answer = qa_chain.invoke(\n",
    "    \"Based on the information within the pdf and NERSC's AI goals what are the most important challenges in achieving these goals\"\n",
    ")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01b98751-3d9c-40d6-9bc5-8454edc0d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once finished end all jobs\n",
    "!scancel $(squeue --me --name=vLLM_RAG --format=\"%i\" -h) $(squeue --me --name=embed_rerank_RAG --format=\"%i\" -h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4b739e-04a0-4dd8-9170-a33629488c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
